!pip install networkx

import json
import pandas as pd
import networkx as nx
import sys

# Set the display option to show all rows
# pd.set_option('display.max_rows', 1000)

# url = sisense_conn.sisense_base_url
# buildCube = sisense_conn.query_building_cube
cube_name = sisense_conn.cube_name
# Uncomment the next line for testing and set to whatever cube/data you are testing
# cube_name = 'Kineo Standard Data Model' # 'DemoDataset'

# Get the OID of the data model - needed for next API call
elasticubes = json.loads(sisense_conn.call_api(http_method="GET", url_suffix="/api/v1/elasticubes/getElasticubes").text)
# print(elasticubes)
model_oid = None

for model in elasticubes:
        if model.get('title') == cube_name:
            model_oid = model.get('oid')
# print(model_oid)

# Log information and exit. Can use this to see info in the build failed log
# sys.exit("test")

# Get the cube schema and find the folder data is being stored in
cubeschema = json.loads(sisense_conn.call_api(http_method="GET", url_suffix="/api/v2/datamodels/"+model_oid+"/schema").text)
# print (cubeschema)

# Initialize folder variable
folder_path = None

# Loop through datasets to find the correct file and extract the folder
for dataset in cubeschema.get("datasets", []):
    connection = dataset.get("connection")
    if connection and connection.get("fileName") == "analytics_course_completions.csv":
        folder_path = connection.get("parameters", {}).get("folder")
        break  # Stop searching once found

if folder_path == None: 

    # Loop through datasets to find the correct file and extract the folder
    for dataset in cubeschema.get("datasets", []):
        connection = dataset.get("connection")
        if connection:
            # Check if schema path ends with or contains the CSV file name
            schema_path = connection.get("schema", "")
            if "analytics_course_completions.csv" in schema_path:
                folder_path = connection.get("parameters", {}).get("folder")
                break

df_result = pd.DataFrame()
FoundFolderPath = False

# Output the result
if folder_path:
    print(f"The folder for 'analytics_course_completions.csv' is: {folder_path}")
    FoundFolderPath = True
else:
    print("File 'analytics_course_completions.csv' not found in the schema.")
    # Return an empty dataframe as this is likely the first cube build and the schema does not yet exist.
    
    columns = ['employee_id', 'manager_id', 'assignment_id','relationship_type']

    # Create an empty DataFrame with the defined columns
    df_result = pd.DataFrame(columns=columns)
	
if FoundFolderPath == True:
    data = pd.read_csv(folder_path + '/analytics_users.csv')
    columns_to_keep = ['User ID', 'Manager ID', 'Assignment ID']
    
    # Create a new DataFrame with only the desired columns
    filtered_data = data[columns_to_keep].copy()
    
    new_column_names = {
        'User ID': 'employee_id',
        'Manager ID': 'manager_id',
        'Assignment ID': 'assignment_id',
    }
    
    # Rename the columns
    filtered_data.rename(columns=new_column_names, inplace=True)
    
    
    
    # Display the filtered data
    # print(filtered_data)
    
    sql_results = filtered_data

if FoundFolderPath == True:
    df = sql_results
    # df[df['employee_id'] == 13]
	
# Setup self relationship
	
if FoundFolderPath == True:
    all_id = pd.DataFrame(pd.concat([df['manager_id'], df['employee_id']]).unique())
    # all_id = all_id[all_id[0] != 'N\A']
    all_id = all_id[pd.notna(all_id[0])]
	
if FoundFolderPath == True:
    self_df = pd.DataFrame()
    self_df['employee_id'] = all_id
    self_df['manager_id'] = all_id
    self_df = pd.merge(self_df, df[['employee_id', 'assignment_id']], on='employee_id', how='left')
    self_df['assignment_id'] = self_df['assignment_id'].fillna(-1) # cater for users with no assignments
    self_df['relationship_type'] = 'self'
    # print(self_df[self_df['employee_id'] == 1952])
	
if FoundFolderPath == True:
    # direct_df = df.query('manager_id != "N\A"').copy()
    direct_df = df[df['manager_id'].notna()].copy()
    
    # Records where manager_id is missing
    na_manager_df = df[df['manager_id'].isna()].copy()
    na_manager_df['manager_id'] = -1
    
    # Combine them
    direct_df = pd.concat([direct_df, na_manager_df], ignore_index=True)
	
if FoundFolderPath == True:
    direct_df['relationship_type'] = 'direct'
    direct_df['assignment_id'] = direct_df['assignment_id'].fillna(-1) # cater for users with no assignments
	# direct_df[direct_df.employee_id == 13]

# Combine self and direct relationships

if FoundFolderPath == True:
    combine_df = pd.concat([self_df, direct_df])
	
# if FoundFolderPath == True:
    # combine_df
    # combine_df[combine_df['employee_id'] == 13]
	
# Expand indirect relationship

# if FoundFolderPath == True:
    # combine_df[combine_df['employee_id'] == 13]
	
if FoundFolderPath == True:
    # Ensure no null assignment IDs
    combine_df = combine_df[combine_df['assignment_id'].notna()]
    
    # Prepare the DataFrame to accumulate results
    results = pd.DataFrame()
    
    # Iterate over each employee and assignment to trace up the management hierarchy
    for _, row in combine_df.iterrows():
        current_employee_id = row['employee_id']
        current_manager_id = row['manager_id']
        current_assignment_id = row['assignment_id']
        current_relationship_type = row['relationship_type']
        visited_managers = set()  # Track visited managers to avoid cycles
    
        # Track up the chain of management
        while current_manager_id and current_manager_id not in visited_managers and current_relationship_type!="self":
            visited_managers.add(current_manager_id)
            manager_info = combine_df[combine_df['employee_id'] == current_manager_id]
            #if manager_info.empty:
            #        break
            # print (manager_info)
            
            for _, manager_info_row in manager_info.iterrows():    
                if manager_info_row['relationship_type']  != "self":                        
                    next_manager_id = manager_info_row['manager_id']
            
                    # Skip if it's a self-managing situation
                    if next_manager_id == current_manager_id:
                        continue
                    
                    # Append the indirect relationship if valid
                    if next_manager_id and next_manager_id not in visited_managers:
                        new_row = pd.DataFrame({
                            'employee_id': [current_employee_id],
                            'manager_id': [next_manager_id],
                            'assignment_id': [current_assignment_id],
                            'relationship_type': ['indirect']
                        })
                        results = pd.concat([results, new_row], ignore_index=True)
            
                    # Update the current manager to the next level manager
                    current_manager_id = next_manager_id
    
    # Remove duplicates and combine with original data
    final_results = pd.concat([combine_df, results]).drop_duplicates(subset=['employee_id', 'manager_id', 'assignment_id'], keep='first').reset_index(drop=True)
    
    # Display results to see all indirect relationships for a specific employee_id
    # print(results[results['employee_id'] == 13])

if FoundFolderPath == True:
    df_result = final_results
	
# df_result[df_result['employee_id'] == 1302]

# display(df_result)
